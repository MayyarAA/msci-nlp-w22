# -*- coding: utf-8 -*-
"""MSCI598Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rdkXIplm13JCLK30ybtjjdyaa6qgURrY
"""
import re
import random
import csv


class TokenizeClass:

  posFilePath = "/content/drive/My Drive/Colab Notebooks/MSCI598/pos.txt"
  stopWordsFilePath = "/content/drive/My Drive/Colab Notebooks/MSCI598/nltkstopwordslist.txt"
  def mainRunner(self):
    #read data from file
    posFileString = self.retriveTextFromFile(self.posFilePath)
    stopWordsList = self.retriveTextFromFile(self.stopWordsFilePath)

    # remove specailchars from wordlists
    wordListWithoutSpecChar = self.removeSpecailCharFromList(posFileString)
    # create hashset to store stopwords
    stopWordsWithOutSpecChar = self.removeSpecailCharFromList(stopWordsList)
    # tokenize wordlist
    tokenizedWordList = self.tokenizeWordList(wordListWithoutSpecChar)
    stopWordsHashSet = self.createHashSetFromWordList(stopWordsWithOutSpecChar)



  # get text from file
  def retriveTextFromFile(filePath):
    with open(filePath) as f:
      posFileString = f.readlines()
      return posFileString


  # 2 Remove the following special characters: !"#$%&()*+/:;<=>@[\\]^`{|}~\t\n
  # remove special char
  def removeSpecailCharFromList(orgWordList):
    wordList = [];
    for curr in orgWordList:
      word = removeSpecailCharFromString(curr)
      wordList.append(word)
    return wordList;


  def removeSpecailCharFromString(orgWord):
    return re.sub(r"[^a-zA-Z0-9]+", ' ', orgWord).strip()

  # create dict to store stopwords
  def createHashSetFromWordList(wordList):
    wordHashSet = set()
    for curr in wordList:
      wordHashSet.add(curr)
    return wordHashSet




  # tokenize wordlist
  def tokenizeWordList(wordList):
    tokenizedWordList = [];
    for word in wordList:
      wordArray = word.split()
      tokenizedWordList.append(wordArray)
    return tokenizedWordList



  # remove stopwords
  def removeStopWords(tokenizedWordList, wordHashSet):
    tokenizedWordListWithoutStopWords = []
    for wordArr in tokenizedWordList:
      wordArrWithoutStopWords = []
      for word in wordArr:
        if word not in wordHashSet:
          wordArrWithoutStopWords.append(word)
      tokenizedWordListWithoutStopWords.append(wordArrWithoutStopWords)
    return tokenizedWordListWithoutStopWords

  print('out' in stopWordsHashSet)

  # --------testing--------------
  testTokenWordList = [['word', 'out', 'jsks', 'tom'], ['word', 'bas', 'kook', 'out']]
  testSet = {"out", "kook"}
  testTokenList = removeStopWords(testTokenWordList, testSet)
  testTokenList

  # 3 Create two versions of your dataset:
  # (3.1) with stopwords

  # (3.2) without stopwords.

  # remove stopwords
  tokenizedWordListWithoutStopWords = removeStopWords(tokenizedWordList, stopWordsHashSet)
  # tokenizedWordListWithoutStopWords.pop(0)

  # 4 Randomly split your data into training (80%), validation (10%) and test (10%) sets

  # randomly split your data into training(80%) and validation(10%) and test(10%) sets

  print(' tokenizedWordListWithoutStopWords len ', len(tokenizedWordListWithoutStopWords))
  # split list w/o stopwords
  random.shuffle(tokenizedWordListWithoutStopWords)
  tokenizedWordListWithoutStopWordsTraining = tokenizedWordListWithoutStopWords[
                                              :int((len(tokenizedWordListWithoutStopWords) + 1) * .8)]
  print(' tokenizedWordListWithoutStopWordsTraining len ', len(tokenizedWordListWithoutStopWordsTraining), ' pop top ',
        tokenizedWordListWithoutStopWordsTraining.pop(0))
  random.shuffle(tokenizedWordListWithoutStopWords)
  tokenizedWordListWithoutStopWordsValidation = tokenizedWordListWithoutStopWords[
                                                :int((len(tokenizedWordListWithoutStopWords) + 1) * .1)]
  print(' tokenizedWordListWithoutStopWordsValidation len ', len(tokenizedWordListWithoutStopWordsValidation),
        ' pop top ', tokenizedWordListWithoutStopWordsValidation.pop(0))
  random.shuffle(tokenizedWordListWithoutStopWords)
  tokenizedWordListWithoutStopWordsTesting = tokenizedWordListWithoutStopWords[
                                             :int((len(tokenizedWordListWithoutStopWords) + 1) * .1)]
  print(' tokenizedWordListWithoutStopWordsTesting len ', len(tokenizedWordListWithoutStopWordsTesting), ' pop top ',
        tokenizedWordListWithoutStopWordsTesting.pop(0))

  # split list w/ stopwords
  random.shuffle(tokenizedWordList)
  tokenizedWordListTraining = tokenizedWordList[:int((len(tokenizedWordList) + 1) * .8)]
  print(' tokenizedWordListTraining len ', len(tokenizedWordListTraining), ' pop top ',
        tokenizedWordListTraining.pop(0))
  random.shuffle(tokenizedWordList)
  tokenizedWordListValidation = tokenizedWordList[:int((len(tokenizedWordList) + 1) * .1)]
  print(' tokenizedWordListValidation len ', len(tokenizedWordListValidation), ' pop top ',
        tokenizedWordListValidation.pop(0))
  random.shuffle(tokenizedWordList)
  tokenizedWordListTesting = tokenizedWordList[:int((len(tokenizedWordList) + 1) * .1)]
  print(' tokenizedWordListTesting len ', len(tokenizedWordListTesting), ' pop top ', tokenizedWordListTesting.pop(0))

  # Create Output files
  def writeToFileWithList(list):
    with open('GFG', 'w') as f:
      write = csv.writer(f)
      write.writerows(list)

  writeToFileWithList(tokenizedWordListTesting)


#2 Remove the following special characters: !"#$%&()*+/:;<=>@[\\]^`{|}~\t\n

#3 Create two versions of your dataset: 
#(3.1) with stopwords 

#(3.2) without stopwords.

#4 Randomly split your data into training (80%), validation (10%) and test (10%) sets