{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSCI598Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import csv"
      ],
      "metadata": {
        "id": "1tirx-IoW33m"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')     \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaTwLzLRM924",
        "outputId": "cb10cfd8-3e64-447a-b73e-9cc5181ddc3d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks/MSCI598\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF2Mt7iKVYFp",
        "outputId": "16b012d4-b74c-490c-dd86-3e50ecc92729"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSCI598Assignment1.ipynb  neg.txt  nltkstopwordslist.txt  pos.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "posFilePath = \"/content/drive/My Drive/Colab Notebooks/MSCI598/pos.txt\"\n",
        "stopWordsFilePath = \"/content/drive/My Drive/Colab Notebooks/MSCI598/nltkstopwordslist.txt\""
      ],
      "metadata": {
        "id": "aW4BnBjFVIVM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get text from file\n",
        "def retriveTextFromFile(filePath):\n",
        "  with open(filePath) as f:\n",
        "    posFileString = f.readlines()\n",
        "    return posFileString\n"
      ],
      "metadata": {
        "id": "R792bSlDUY4e"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posFileString = retriveTextFromFile(posFilePath)\n",
        "stopWordsList = retriveTextFromFile(stopWordsFilePath)"
      ],
      "metadata": {
        "id": "R9Hzi0ZKXTJ3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Remove the following special characters: !\"#$%&()*+/:;<=>@[\\\\]^`{|}~\\t\\n\n",
        "#remove special char\n",
        "def removeSpecailCharFromList(orgWordList):\n",
        "  wordList = [];\n",
        "  for curr in orgWordList:\n",
        "    word = removeSpecailCharFromString(curr)\n",
        "    wordList.append(word)\n",
        "  return wordList;\n",
        "#posFileString.pop(0)\n",
        "\n",
        "def removeSpecailCharFromString(orgWord):\n",
        "  return re.sub(r\"[^a-zA-Z0-9]+\", ' ', orgWord).strip()\n"
      ],
      "metadata": {
        "id": "uKlrKLGWWeAY"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create dict to store stopwords\n",
        "def createHashSetFromWordList(wordList):\n",
        "    wordHashSet=set()\n",
        "    for curr in wordList:\n",
        "      wordHashSet.add(curr)\n",
        "    return wordHashSet"
      ],
      "metadata": {
        "id": "SN4w3Uy6inm5"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove specailchars from wordlists\n",
        "wordListWithoutSpecChar = removeSpecailCharFromList(posFileString)\n",
        "stopWordsWithOutSpecChar = removeSpecailCharFromList(stopWordsList)\n",
        "#create hashset to store stopwords\n",
        "stopWordsHashSet = createHashSetFromWordList(stopWordsWithOutSpecChar)\n",
        "len(stopWordsHashSet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsyH4PoEWtzy",
        "outputId": "fd80f0c0-323f-4347-add8-8759b9cff4f0"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize wordlist\n",
        "def tokenizeWordList(wordList):\n",
        "  tokenizedWordList = [];\n",
        "  for word in wordList:\n",
        "    wordArray = word.split()\n",
        "    tokenizedWordList.append(wordArray)\n",
        "  return tokenizedWordList"
      ],
      "metadata": {
        "id": "AvvJM43xksno"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize wordlist\n",
        "tokenizedWordList = tokenizeWordList(wordListWithoutSpecChar)\n",
        "tokenizedWordList.pop(0)"
      ],
      "metadata": {
        "id": "EA23TOPukkIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oLcO4aqyiS7P"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stopwords\n",
        "def removeStopWords(tokenizedWordList,wordHashSet):    \n",
        "  tokenizedWordListWithoutStopWords = []\n",
        "  for wordArr in tokenizedWordList: \n",
        "    wordArrWithoutStopWords = []   \n",
        "    for word in wordArr:                  \n",
        "      if word not in wordHashSet:  \n",
        "        wordArrWithoutStopWords.append(word)     \n",
        "    tokenizedWordListWithoutStopWords.append(wordArrWithoutStopWords)\n",
        "  return tokenizedWordListWithoutStopWords"
      ],
      "metadata": {
        "id": "SMKNPYjxV3ny"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('out' in stopWordsHashSet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glwBI6zdmuoe",
        "outputId": "48895497-7f9a-4e84-9182-227c23b96507"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--------testing--------------\n",
        "testTokenWordList= [['word','out','jsks','tom'],['word','bas','kook','out']]\n",
        "testSet = {\"out\",\"kook\"}\n",
        "testTokenList = removeStopWords(testTokenWordList,testSet)\n",
        "testTokenList"
      ],
      "metadata": {
        "id": "GI6SsH1bnizg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Create two versions of your dataset: \n",
        "#(3.1) with stopwords \n",
        "\n",
        "#(3.2) without stopwords. \n",
        "\n",
        "#remove stopwords \n",
        "tokenizedWordListWithoutStopWords = removeStopWords(tokenizedWordList,stopWordsHashSet)\n",
        "#tokenizedWordListWithoutStopWords.pop(0)"
      ],
      "metadata": {
        "id": "Hw3Xnuvmle4Q"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Randomly split your data into training (80%), validation (10%) and test (10%) sets\n",
        "\n",
        "#randomly split your data into training(80%) and validation(10%) and test(10%) sets \n",
        "\n",
        "print(' tokenizedWordListWithoutStopWords len ', len(tokenizedWordListWithoutStopWords))\n",
        "#split list w/o stopwords\n",
        "random.shuffle(tokenizedWordListWithoutStopWords)\n",
        "tokenizedWordListWithoutStopWordsTraining =  tokenizedWordListWithoutStopWords[:int((len(tokenizedWordListWithoutStopWords)+1)*.8)]\n",
        "print(' tokenizedWordListWithoutStopWordsTraining len ', len(tokenizedWordListWithoutStopWordsTraining), ' pop top ',tokenizedWordListWithoutStopWordsTraining.pop(0))\n",
        "random.shuffle(tokenizedWordListWithoutStopWords)\n",
        "tokenizedWordListWithoutStopWordsValidation =  tokenizedWordListWithoutStopWords[:int((len(tokenizedWordListWithoutStopWords)+1)*.1)]\n",
        "print(' tokenizedWordListWithoutStopWordsValidation len ', len(tokenizedWordListWithoutStopWordsValidation), ' pop top ',tokenizedWordListWithoutStopWordsValidation.pop(0))\n",
        "random.shuffle(tokenizedWordListWithoutStopWords)\n",
        "tokenizedWordListWithoutStopWordsTesting =  tokenizedWordListWithoutStopWords[:int((len(tokenizedWordListWithoutStopWords)+1)*.1)]\n",
        "print(' tokenizedWordListWithoutStopWordsTesting len ', len(tokenizedWordListWithoutStopWordsTesting), ' pop top ',tokenizedWordListWithoutStopWordsTesting.pop(0) )\n",
        "\n",
        "#split list w/ stopwords\n",
        "random.shuffle(tokenizedWordList)\n",
        "tokenizedWordListTraining =  tokenizedWordList[:int((len(tokenizedWordList)+1)*.8)]\n",
        "print(' tokenizedWordListTraining len ', len(tokenizedWordListTraining), ' pop top ',tokenizedWordListTraining.pop(0))\n",
        "random.shuffle(tokenizedWordList)\n",
        "tokenizedWordListValidation =  tokenizedWordList[:int((len(tokenizedWordList)+1)*.1)]\n",
        "print(' tokenizedWordListValidation len ', len(tokenizedWordListValidation), ' pop top ',tokenizedWordListValidation.pop(0))\n",
        "random.shuffle(tokenizedWordList)\n",
        "tokenizedWordListTesting =  tokenizedWordList[:int((len(tokenizedWordList)+1)*.1)]\n",
        "print(' tokenizedWordListTesting len ', len(tokenizedWordListTesting), ' pop top ',tokenizedWordListTesting.pop(0) )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlsYcjz5pB8Z",
        "outputId": "9bd44d29-80ec-4f5b-dc1c-3e3b62a6a680"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " tokenizedWordListWithoutStopWords len  399999\n",
            " tokenizedWordListWithoutStopWordsTraining len  320000  pop top  ['No', 'need', 'spend', 'extra', 'money', 'expensive', 'press']\n",
            " tokenizedWordListWithoutStopWordsValidation len  40000  pop top  ['I', 'would', 'definitely', 'recommend', 'looking', 'high', 'quality', 'bumper', 'want', 'spend', '30', 'apple', 'one']\n",
            " tokenizedWordListWithoutStopWordsTesting len  40000  pop top  ['I', 'bought', 'second', 'one', 'original', 'eventually', 'wears', 'shouldn', 'time', 'soon']\n",
            " tokenizedWordListTraining len  320000  pop top  ['The', 'pot', 'is', 'big', 'enough', 'to', 'hold', 'a', 'pound', 'of', 'spaghetti']\n",
            " tokenizedWordListValidation len  40000  pop top  ['overall', 'not', 'a', 'bad', 'case', 'for', 'the', 'money']\n",
            " tokenizedWordListTesting len  40000  pop top  ['Sophisticated', 'interface', 'talks', 'to', 'you', 'about', 'volume', 'and', 'connection', 'status', 'and', 'puts', 'a', 'battery', 'bar', 'on', 'top', 'of', 'the', 'iPhone', 'screen']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Output files\n",
        "def writeToFileWithList(list):\n",
        "  with open('GFG','w') as f:\n",
        "    write = csv.writer(f)\n",
        "    write.writerows(list)"
      ],
      "metadata": {
        "id": "UpEy__6Ot4JO"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writeToFileWithList(tokenizedWordListTesting)"
      ],
      "metadata": {
        "id": "IXirRMZ9ui5b"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Remove the following special characters: !\"#$%&()*+/:;<=>@[\\\\]^`{|}~\\t\\n"
      ],
      "metadata": {
        "id": "om-W1ZdBMfQC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Create two versions of your dataset: \n",
        "#(3.1) with stopwords \n",
        "\n",
        "#(3.2) without stopwords. "
      ],
      "metadata": {
        "id": "rfEuKHE4MifR"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Randomly split your data into training (80%), validation (10%) and test (10%) sets"
      ],
      "metadata": {
        "id": "gOl-VVlutaQ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}