# -*- coding: utf-8 -*-
"""MSCI598Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rdkXIplm13JCLK30ybtjjdyaa6qgURrY
"""

import re
import random
import csv


posFilePath = "./pos.txt"
stopWordsFilePath = "./nltkstopwordslist.txt"
def runApp(fileToParse,stopWordsFilePathFull):
  posFileString = retriveTextFromFile(posFilePath)
  stopWordsList = retriveTextFromFile(stopWordsFilePath)

#get text from file
def retriveTextFromFile(filePath):
  with open(filePath) as f:
    posFileString = f.readlines()
    return posFileString

posFileString = retriveTextFromFile(posFilePath)
stopWordsList = retriveTextFromFile(stopWordsFilePath)

#2 Remove the following special characters: !"#$%&()*+/:;<=>@[\\]^`{|}~\t\n
#remove special char
def removeSpecailCharFromList(orgWordList):
  wordList = [];
  for curr in orgWordList:
    word = removeSpecailCharFromString(curr)
    wordList.append(word)
  return wordList;


def removeSpecailCharFromString(orgWord):
  return re.sub(r"[^a-zA-Z0-9]+", ' ', orgWord).strip()

#create hashset to store stopwords
def createHashSetFromWordList(wordList):
    wordHashSet=set()
    for curr in wordList:
      wordHashSet.add(curr)
    return wordHashSet

#remove specailchars from wordlists
wordListWithoutSpecChar = removeSpecailCharFromList(posFileString)
stopWordsWithOutSpecChar = removeSpecailCharFromList(stopWordsList)
#create hashset to store stopwords
stopWordsHashSet = createHashSetFromWordList(stopWordsWithOutSpecChar)
len(stopWordsHashSet)

#tokenize wordlist
def tokenizeWordList(wordList):
  tokenizedWordList = [];
  for word in wordList:
    wordArray = word.split()
    tokenizedWordList.append(wordArray)
  return tokenizedWordList

#tokenize wordlist
tokenizedWordList = tokenizeWordList(wordListWithoutSpecChar)
tokenizedWordList.pop(0)

#remove stopwords
def removeStopWords(tokenizedWordList,wordHashSet):    
  tokenizedWordListWithoutStopWords = []
  for wordArr in tokenizedWordList: 
    wordArrWithoutStopWords = []   
    for word in wordArr:                  
      if word not in wordHashSet:  
        wordArrWithoutStopWords.append(word)     
    tokenizedWordListWithoutStopWords.append(wordArrWithoutStopWords)
  return tokenizedWordListWithoutStopWords

print('out' in stopWordsHashSet)

#--------testing--------------
testTokenWordList= [['word','out','jsks','tom'],['word','bas','kook','out']]
testSet = {"out","kook"}
testTokenList = removeStopWords(testTokenWordList,testSet)
testTokenList

#3 Create two versions of your dataset: 
#(3.1) with stopwords 

#(3.2) without stopwords. 

#remove stopwords 
tokenizedWordListWithoutStopWords = removeStopWords(tokenizedWordList,stopWordsHashSet)
#tokenizedWordListWithoutStopWords.pop(0)
len(tokenizedWordListWithoutStopWords)

#randomly split your data into training(80%) and validation(10%) and test(10%) sets 

print(' tokenizedWordListWithoutStopWords len ', len(tokenizedWordListWithoutStopWords))
#split list w/o stopwords
random.shuffle(tokenizedWordListWithoutStopWords)
tokenizedWordListWithoutStopWordsTraining =  tokenizedWordListWithoutStopWords[:int((len(tokenizedWordListWithoutStopWords)+1)*.8)]
print(' tokenizedWordListWithoutStopWordsTraining len ', len(tokenizedWordListWithoutStopWordsTraining), ' pop top ',tokenizedWordListWithoutStopWordsTraining.pop(0))
random.shuffle(tokenizedWordListWithoutStopWords)
tokenizedWordListWithoutStopWordsValidation =  tokenizedWordListWithoutStopWords[int((len(tokenizedWordListWithoutStopWords)+1)*.8):int((len(tokenizedWordListWithoutStopWords)+1)*.9)]
print(' tokenizedWordListWithoutStopWordsValidation len ', len(tokenizedWordListWithoutStopWordsValidation), ' pop top ',tokenizedWordListWithoutStopWordsValidation.pop(0))
random.shuffle(tokenizedWordListWithoutStopWords)
tokenizedWordListWithoutStopWordsTesting =  tokenizedWordListWithoutStopWords[int((len(tokenizedWordListWithoutStopWords)+1)*.9):int((len(tokenizedWordListWithoutStopWords)+1)*1)]
print(' tokenizedWordListWithoutStopWordsTesting len ', len(tokenizedWordListWithoutStopWordsTesting), ' pop top ',tokenizedWordListWithoutStopWordsTesting.pop(0) )

#split list w/ stopwords
random.shuffle(tokenizedWordList)
print(' tokenizedWordList len ', len(tokenizedWordList))
tokenizedWordListTraining =  tokenizedWordList[:int((len(tokenizedWordList)+1)*.8)]
print(' tokenizedWordListTraining len ', len(tokenizedWordListTraining), ' pop top ',tokenizedWordListTraining.pop(0))
random.shuffle(tokenizedWordList)
tokenizedWordListValidation =  tokenizedWordList[int((len(tokenizedWordList)+1)*.8):int((len(tokenizedWordList)+1)*.9)]
print(' tokenizedWordListValidation len ', len(tokenizedWordListValidation), ' pop top ',tokenizedWordListValidation.pop(0))
random.shuffle(tokenizedWordList)
tokenizedWordListTesting =  tokenizedWordList[int((len(tokenizedWordList)+1)*.9):int((len(tokenizedWordList)+1)*1)]
print(' tokenizedWordListTesting len ', len(tokenizedWordListTesting), ' pop top ',tokenizedWordListTesting.pop(0) )


#Create Output files
def writeToFileWithListV2(list,filename):
  outputFile = open(filename,"w")
  csvWriter = csv.writer(outputFile);
  for line in list:
    #lineString = ''.join(str(word) for word in line)
    csvWriter.writerow(line)
  outputFile.close()


writeToFileWithListV2(tokenizedWordListWithoutStopWords,"./data/tokenizedWordListWithoutStopWords.csv")

#2 Remove the following special characters: !"#$%&()*+/:;<=>@[\\]^`{|}~\t\n

#3 Create two versions of your dataset: 
#(3.1) with stopwords 

#(3.2) without stopwords.

#4 Randomly split your data into training (80%), validation (10%) and test (10%) sets